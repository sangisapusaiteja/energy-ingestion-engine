# =============================================================================
# POSTGRESQL TUNING — Energy Ingestion Engine
# =============================================================================
# Baseline: 32GB RAM, SSD storage, 28M inserts/day
# Profile: Write-heavy ingestion + read-heavy analytics
# =============================================================================

# ── Memory ───────────────────────────────────────────────────────────────────

# 25% of system RAM. Postgres's internal page cache.
# The hot store (~20K rows) lives entirely in here.
# Active partition indexes stay cached after first access.
shared_buffers = 256MB

# Tell the planner how much total cache is available (OS page cache + shared_buffers).
# 75% of system RAM. Affects cost estimates, not actual allocation.
effective_cache_size = 1GB

# Per-operation memory for sorts, hash joins, and aggregations.
# Our analytics queries aggregate ~1440 rows — 64MB is generous.
# WARNING: This is per-operation, not per-connection. With 100 connections
# running complex queries simultaneously: 100 × 64MB = 6.4GB potential peak.
work_mem = 16MB

# Memory for VACUUM, CREATE INDEX, ALTER TABLE ADD FK.
# Large value speeds up partition index builds and autovacuum.
maintenance_work_mem = 256MB

# ── WAL (Write-Ahead Log) ───────────────────────────────────────────────────

# replica: enables streaming replication to read replicas.
wal_level = replica

# Max WAL accumulated before forcing a checkpoint.
# At 28M inserts/day with batched writes, WAL generation is heavy.
# 4GB gives the checkpointer room to spread writes.
max_wal_size = 4GB

# Minimum WAL retained. Keeps enough for replica catch-up.
min_wal_size = 1GB

# Spread checkpoint I/O over 90% of the checkpoint interval.
# Prevents I/O spikes at checkpoint boundaries.
checkpoint_completion_target = 0.9

# ── Query Planner ────────────────────────────────────────────────────────────

# SSD-optimized. Default (4.0) assumes HDD seek latency.
# On SSD, random reads are nearly as fast as sequential.
random_page_cost = 1.1

# Number of concurrent I/O requests the OS can handle.
# SSDs handle hundreds of parallel reads. Default (1) is for spinning disks.
effective_io_concurrency = 200

# ── Parallelism ──────────────────────────────────────────────────────────────

# Max worker processes for parallel queries.
# Analytics queries on large partitions benefit from parallel seq scans.
max_parallel_workers_per_gather = 4

# Total parallel workers available system-wide.
max_parallel_workers = 8

# Enable parallel on partitioned table appends.
# Critical for fleet-summary queries scanning multiple partitions.
enable_partitionwise_aggregate = on

# ── Connections ──────────────────────────────────────────────────────────────

# Accept connections from all interfaces (required for Docker networking).
listen_addresses = '*'

# Direct connections are handled by PgBouncer in front.
# This should be slightly above PgBouncer's server pool + superuser reserve.
max_connections = 120

# Reserved for admin/monitoring connections that bypass PgBouncer.
superuser_reserved_connections = 5

# ── Autovacuum ───────────────────────────────────────────────────────────────

# More workers for partitioned tables — each partition is vacuumed independently.
autovacuum_max_workers = 6

# Check for vacuum-eligible tables more frequently.
autovacuum_naptime = 30s

# Lower threshold: start vacuum sooner on hot tables.
# vehicle_current_status and meter_current_status get 20K upserts/min.
autovacuum_vacuum_threshold = 50
autovacuum_vacuum_scale_factor = 0.05

# Lower threshold for ANALYZE too — keeps statistics fresh for the planner.
autovacuum_analyze_threshold = 50
autovacuum_analyze_scale_factor = 0.02

# ── Logging (Production) ────────────────────────────────────────────────────

# Log slow queries (> 500ms). Our target is <10ms for all endpoints.
log_min_duration_statement = 500

# Log autovacuum runs longer than 250ms for monitoring.
log_autovacuum_min_duration = 250

# Log lock waits — early warning for contention.
log_lock_waits = on

# ── Misc ─────────────────────────────────────────────────────────────────────

# Timezone for timestamptz interpretation.
timezone = 'UTC'

# JIT compilation: disabled for OLTP. JIT adds planning overhead
# that only pays off on long-running analytical queries (>100ms).
# Our queries are <10ms — JIT would slow them down.
jit = off
